{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/n/data1/hms/dbmi/oconnor/lab/shz311/pangenome/graph_var\")\n",
    "\n",
    "from graph_var.utils import load_graph_from_pkl, merge_dicts, log_action\n",
    "from graph_var.graph import PangenomeGraph\n",
    "from graph_var.evaluating_functions import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'v1'\n",
    "ref_name = 'CHM13'\n",
    "\n",
    "graph_obj_dir = f\"/n/data1/hms/dbmi/oconnor/lab/shz311/pangenome/Graph_objs_{version}{'_chm13' if ref_name == 'CHM13' else ''}\"\n",
    "raw_vcf_dir = f\"/n/data1/hms/dbmi/oconnor/lab/shz311/pangenome/VCFs_chr\"\n",
    "graph_vcf_dir = f\"/n/data1/hms/dbmi/oconnor/lab/shz311/pangenome/VCFs_{version}{'_chm13' if ref_name == 'CHM13' else ''}\"\n",
    "\n",
    "ref_tree_dir = f\"/n/data1/hms/dbmi/oconnor/lab/shz311/pangenome/Data/reference_tree_gfa_{version}{'_chm13' if ref_name == 'CHM13' else ''}\"\n",
    "gfa_dir = f\"/n/data1/hms/dbmi/oconnor/lab/shz311/pangenome/Data/chromosome_gfa_{version}{'_chm13' if ref_name == 'CHM13' else ''}\"\n",
    "snarl_dir = f\"/n/data1/hms/dbmi/oconnor/lab/shz311/pangenome/Data/chr_snarls_{version}{'_chm13' if ref_name == 'CHM13' else ''}\"\n",
    "bubble_summary_dir = f\"/n/data1/hms/dbmi/oconnor/lab/shz311/pangenome/Bubble_summary_{version}{'_chm13' if ref_name == 'CHM13' else ''}\"\n",
    "\n",
    "var_summary_dir = f\"/n/data1/hms/dbmi/oconnor/lab/shz311/pangenome/Stats_chr_{version}{'_chm13' if ref_name == 'CHM13' else ''}\"\n",
    "data_vis_dir = f\"/n/data1/hms/dbmi/oconnor/lab/shz311/pangenome/Data_visualization_{version}{'_chm13' if ref_name == 'CHM13' else ''}\"\n",
    "\n",
    "region_dir = f\"/n/data1/hms/dbmi/oconnor/lab/shz311/pangenome/Region_files\"\n",
    "\n",
    "mode = 'AT'\n",
    "exclude_terminus = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr_set = 'Y'\n",
    "\n",
    "if chr_set == 'autosome':\n",
    "    chr_list = list(range(1, 23))\n",
    "elif chr_set == 'X':\n",
    "    chr_list = ['X']\n",
    "elif chr_set == 'Y':\n",
    "    chr_list = ['Y']\n",
    "else:\n",
    "    raise ValueError(\"chr_set must be one of ['autosome', 'X', 'Y']\")\n",
    "\n",
    "num_chr = len(chr_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Summary for all variant edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variant_edges_summary_from_dict(var_list: list, var_dict: dict):\n",
    "    summary_dict = dict()\n",
    "    for edge in sorted(list(var_list)):\n",
    "        summary_dict[var_dict[edge]] = summary_dict.get(var_dict[edge], 0) + 1\n",
    "    summary_dict['Total'] = len(var_list)\n",
    "    return summary_dict\n",
    "\n",
    "def prepare_dataframe_dict(var_dict):\n",
    "    return pd.DataFrame({\n",
    "        \"Variant Type\": ['SNP', 'MNP', 'Insertion', 'Deletion', 'Replacement', 'Inversion', 'Duplication', 'Total'],\n",
    "        \"Count\": [\n",
    "                  var_dict.get('SNP', 0),\n",
    "                  var_dict.get('MNP', 0),\n",
    "                  var_dict.get('INS', 0),\n",
    "                  var_dict.get('DEL', 0),\n",
    "                  var_dict.get('REP', 0),\n",
    "                  var_dict.get('INV', 0),\n",
    "                  var_dict.get('DUP', 0),\n",
    "                  var_dict.get('Total', 0),\n",
    "        ]\n",
    "    })\n",
    "\n",
    "def comprehensive_summary(graph_vcf_input: Union[str, pd.DataFrame], \n",
    "                          chrom_label: str, \n",
    "                          tandem_repeat: bool=False):\n",
    "    variant_sets = defaultdict(set)\n",
    "    var_dict = dict()\n",
    "\n",
    "    if isinstance(graph_vcf_input, str):\n",
    "        row_iter = read_vcf_line_by_line(graph_vcf_input)\n",
    "    elif isinstance(graph_vcf_input, pd.DataFrame):\n",
    "        row_iter = graph_vcf_input.to_dict(orient=\"records\")\n",
    "    else:\n",
    "        raise ValueError(\"Input must be either a path to VCF or a pandas DataFrame.\")\n",
    "\n",
    "    for row in row_iter:\n",
    "        edge = row['ID']\n",
    "        info_dict = {k: v for k, v in (field.split('=') for field in row['INFO'].split(';') if '=' in field)}\n",
    "        var_type = info_dict['VT']\n",
    "        nearly_identical = int(info_dict['NIA'])\n",
    "        allele_count = int(info_dict['AC']) if var_type == 'INV' else min(int(info_dict['RC']), int(info_dict['AC']))\n",
    "        ref_allele = row['REF'] if row['REF'] != '.' else info_dict['NR']\n",
    "        alt_allele = row['ALT']\n",
    "        allele_length = len(ref_allele) + len(alt_allele)\n",
    "        on_linear = (int(info_dict['DR'].split(',')[1]) == 0)\n",
    "\n",
    "        is_repeat = info_dict['TR_MOTIF'] != '.'\n",
    "\n",
    "        if tandem_repeat and not is_repeat:\n",
    "            continue\n",
    "\n",
    "        var_dict[edge] = var_type\n",
    "        variant_sets['All'].add(edge)\n",
    "        variant_sets['Linear' if on_linear else 'Off_Linear'].add(edge)\n",
    "        variant_sets['Small' if allele_length < 50 or nearly_identical else 'Large'].add(edge)\n",
    "        variant_sets['Common' if allele_count >= 5 else 'Uncommon'].add(edge)\n",
    "\n",
    "    # Generate combinations\n",
    "    for a in ['Linear', 'Off_Linear']:\n",
    "        for b in ['Small', 'Large']:\n",
    "            variant_sets[f'{a}_{b}'] = variant_sets[a].intersection(variant_sets[b])\n",
    "        for c in ['Common', 'Uncommon']:\n",
    "            variant_sets[f'{a}_{c}'] = variant_sets[a].intersection(variant_sets[c])\n",
    "    for b in ['Small', 'Large']:\n",
    "        for c in ['Common', 'Uncommon']:\n",
    "            variant_sets[f'{b}_{c}'] = variant_sets[b].intersection(variant_sets[c])\n",
    "            for a in ['Linear', 'Off_Linear']:\n",
    "                variant_sets[f'{a}_{b}_{c}'] = variant_sets[a].intersection(variant_sets[f'{b}_{c}'])\n",
    "\n",
    "    # Construct DataFrames\n",
    "    all_var_df = prepare_dataframe_dict(variant_edges_summary_from_dict(variant_sets['All'], var_dict))\n",
    "    summary_dict = {'CHROM': [chrom_label] * len(all_var_df), 'Variant Type': all_var_df['Variant Type']}\n",
    "    for key, variant_set in variant_sets.items():\n",
    "        df = prepare_dataframe_dict(variant_edges_summary_from_dict(variant_set, var_dict))\n",
    "        summary_dict[key+\"_Variants\"] = df['Count']\n",
    "\n",
    "    return pd.DataFrame(summary_dict, columns=[\n",
    "        \"CHROM\",\n",
    "        \"Variant Type\",\n",
    "        \"All_Variants\",\n",
    "        \"Linear_Variants\",\n",
    "        \"Off_Linear_Variants\",\n",
    "        \"Small_Variants\",\n",
    "        \"Large_Variants\",\n",
    "        \"Common_Variants\",\n",
    "        \"Uncommon_Variants\",\n",
    "        \"Linear_Small_Variants\",\n",
    "        \"Linear_Large_Variants\",\n",
    "        \"Off_Linear_Small_Variants\",\n",
    "        \"Off_Linear_Large_Variants\",\n",
    "        \"Linear_Common_Variants\",\n",
    "        \"Linear_Uncommon_Variants\",\n",
    "        \"Off_Linear_Common_Variants\",\n",
    "        \"Off_Linear_Uncommon_Variants\",\n",
    "        \"Small_Common_Variants\",\n",
    "        \"Small_Uncommon_Variants\",\n",
    "        \"Large_Common_Variants\",\n",
    "        \"Large_Uncommon_Variants\",\n",
    "        \"Linear_Small_Common_Variants\",\n",
    "        \"Linear_Large_Common_Variants\",\n",
    "        \"Linear_Small_Uncommon_Variants\",\n",
    "        \"Linear_Large_Uncommon_Variants\",\n",
    "        \"Off_Linear_Small_Common_Variants\",\n",
    "        \"Off_Linear_Large_Common_Variants\",\n",
    "        \"Off_Linear_Small_Uncommon_Variants\",\n",
    "        \"Off_Linear_Large_Uncommon_Variants\"\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.49s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(chr_list):\n",
    "    graph_vcf_path = f\"{graph_vcf_dir}/graph_chr{i}{'_no_terminus' if exclude_terminus else ''}.vcf\"\n",
    "    comp_var_summary_df = comprehensive_summary(graph_vcf_path, f\"chr{i}\")\n",
    "    comp_var_summary_df.to_csv(f\"{var_summary_dir}/comprehensive_variant_summary_for_chr{i}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hatch-marked for repeated variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.70s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(chr_list):\n",
    "    graph_vcf_path = f\"{graph_vcf_dir}/graph_chr{i}{'_no_terminus' if exclude_terminus else ''}.vcf\"\n",
    "    comp_var_summary_df = comprehensive_summary(graph_vcf_path, f\"chr{i}\", tandem_repeat=True)\n",
    "    comp_var_summary_df.to_csv(f\"{var_summary_dir}/comprehensive_repeated_variant_summary_for_chr{i}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Summary for variant edges based on annotated regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other_difficult_bed = \"/n/data1/hms/dbmi/oconnor/lab/shz311/pangenome/Region_files/GRCh38_subtract_difficult_segdup.bed\"\n",
    "easy_bed = f\"{region_dir}/GRCh38_notinalldifficultregions.bed\"\n",
    "segdup_bed = f\"{region_dir}/GRCh38_segdups.bed\"\n",
    "\n",
    "TRregion_bed = f\"{region_dir}/adotto_TRregions_v1.2.1.types.bed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raw_vcf(vcf_df):\n",
    "    def split_vcf_row_minimal(row):\n",
    "        ACs = row['INFO'].split(';')[0].split('=')[1].split(',')\n",
    "        alts = row['ALT'].split(',')\n",
    "        return [{key if key != 'INFO' else 'AC': row[key] if key != 'ALT' and key != 'INFO' else alt if key == 'ALT' else AC \\\n",
    "            for key in row.keys()} for alt, AC in zip(alts, ACs)]\n",
    "\n",
    "    split_rows = []\n",
    "    for i in range(len(vcf_df)):\n",
    "        split_rows.extend(split_vcf_row_minimal(vcf_df.iloc[i]))\n",
    "\n",
    "    splited_vcf_df = pd.DataFrame(split_rows)\n",
    "    return splited_vcf_df[['#CHROM', 'POS', 'REF', 'ALT', 'AC']]\n",
    "\n",
    "def process_ourvcf(graph_vcf_df):\n",
    "    def find_type(x):\n",
    "        info_dict = {attr.split('=')[0]: attr.split('=')[1] for attr in x.split(';') if '=' in attr}\n",
    "        return info_dict['VT']\n",
    "    graph_vcf_df['Variant_Type'] = graph_vcf_df['INFO'].apply(find_type)\n",
    "    def if_linear(x):\n",
    "        info_dict = {attr.split('=')[0]: attr.split('=')[1] for attr in x.split(';') if '=' in attr}\n",
    "        return int(info_dict['DR'].split(',')[1]) == 0\n",
    "    graph_vcf_df['Linear'] = graph_vcf_df['INFO'].apply(if_linear)\n",
    "    def find_ac(x):\n",
    "        info_dict = {attr.split('=')[0]: attr.split('=')[1] for attr in x.split(';') if '=' in attr}\n",
    "        ac = min(int(info_dict['AC']), int(info_dict['RC'])) if info_dict['VT'] != 'INV' else int(info_dict['AC'])\n",
    "        return ac\n",
    "    #graph_vcf_df['AC'] = graph_vcf_df.apply(lambda x: sum([int(AC) for sample in sample_cols for AC in x[sample].split('|')]), axis=1)\n",
    "    graph_vcf_df['AC'] = graph_vcf_df['INFO'].apply(find_ac)\n",
    "    return graph_vcf_df[['#CHROM', 'POS', 'REF', 'ALT', 'AC', 'Linear', 'Variant_Type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_vcf_filtered_by_region(chr_id, raw_vcf=True, graph_vcf=True):\n",
    "    rawvcf_path = f\"{raw_vcf_dir}/hprc-v1.1-mc-grch38.raw_{chr_id}.vcf\"\n",
    "    vcfwave_path = f\"{raw_vcf_dir}/hprc-v1.1-mc-grch38.vcfbub.a100k.wave_{chr_id}.vcf\"\n",
    "    graph_vcf_path = f\"{graph_vcf_dir}/graph_{chr_id}{'_no_terminus' if exclude_terminus else ''}.vcf\"\n",
    "\n",
    "    easy_region = get_interval_tree_from_bed(easy_bed, chr_id)\n",
    "    segdup_region = get_interval_tree_from_bed(segdup_bed, chr_id)\n",
    "    # diff_region = get_interval_tree_from_bed(other_difficult_bed, chr_id)\n",
    "\n",
    "    if raw_vcf:\n",
    "        raw_vcf_df = read_vcf_to_dataframe(rawvcf_path)\n",
    "        raw_vcf_df = process_raw_vcf(raw_vcf_df)\n",
    "\n",
    "        split_raw_vcf_df = raw_vcf_df\n",
    "\n",
    "        raw_easy_bool = split_raw_vcf_df['POS'].apply(lambda x: len(easy_region[int(x)]) > 0)\n",
    "        raw_segdup_bool = split_raw_vcf_df['POS'].apply(lambda x: len(segdup_region[int(x)]) > 0)\n",
    "        raw_diff_bool = ~raw_easy_bool & ~raw_segdup_bool \n",
    "\n",
    "        easy_raw_vcf_df = split_raw_vcf_df[raw_easy_bool]\n",
    "        segdup_raw_vcf_df = split_raw_vcf_df[raw_segdup_bool]\n",
    "        diff_raw_vcf_df = split_raw_vcf_df[raw_diff_bool]\n",
    "\n",
    "        wave_vcf_df = read_vcf_to_dataframe(vcfwave_path)\n",
    "        wave_vcf_df = process_raw_vcf(wave_vcf_df)\n",
    "\n",
    "        split_wave_vcf_df = wave_vcf_df\n",
    "\n",
    "        wave_easy_bool = split_wave_vcf_df['POS'].apply(lambda x: len(easy_region[int(x)]) > 0)\n",
    "        wave_segdup_bool = split_wave_vcf_df['POS'].apply(lambda x: len(segdup_region[int(x)]) > 0)\n",
    "        wave_diff_bool = ~wave_easy_bool & ~wave_segdup_bool \n",
    "\n",
    "        easy_wave_vcf_df = split_wave_vcf_df[wave_easy_bool]\n",
    "        segdup_wave_vcf_df = split_wave_vcf_df[wave_segdup_bool]\n",
    "        diff_wave_vcf_df = split_wave_vcf_df[wave_diff_bool]\n",
    "    else:\n",
    "        easy_raw_vcf_df = None\n",
    "        segdup_raw_vcf_df = None\n",
    "        diff_raw_vcf_df = None\n",
    "\n",
    "        easy_wave_vcf_df = None\n",
    "        segdup_wave_vcf_df = None\n",
    "        diff_wave_vcf_df = None\n",
    "\n",
    "    if graph_vcf:\n",
    "        graph_vcf_df = read_vcf_to_dataframe(graph_vcf_path)\n",
    "        graph_vcf_df = process_ourvcf(graph_vcf_df)\n",
    "        simple_graph_vcf_df = graph_vcf_df\n",
    "\n",
    "        graph_easy_bool = simple_graph_vcf_df['POS'].apply(lambda x: len(easy_region[int(x)]) > 0)\n",
    "        graph_segdup_bool = simple_graph_vcf_df['POS'].apply(lambda x: len(segdup_region[int(x)]) > 0)\n",
    "        graph_diff_bool = ~graph_easy_bool & ~graph_segdup_bool\n",
    "\n",
    "        easy_graph_vcf_df = simple_graph_vcf_df[graph_easy_bool]\n",
    "        segdup_graph_vcf_df = simple_graph_vcf_df[graph_segdup_bool]\n",
    "        diff_graph_vcf_df = simple_graph_vcf_df[graph_diff_bool]\n",
    "    else:\n",
    "        easy_graph_vcf_df = None\n",
    "        segdup_graph_vcf_df = None\n",
    "        diff_graph_vcf_df = None\n",
    "    \n",
    "    return {\n",
    "        \"easy_raw\": easy_raw_vcf_df,\n",
    "        \"segdup_raw\": segdup_raw_vcf_df,\n",
    "        \"diff_raw\": diff_raw_vcf_df,\n",
    "        \"easy_wave\": easy_wave_vcf_df,\n",
    "        \"segdup_wave\": segdup_wave_vcf_df,\n",
    "        \"diff_wave\": diff_wave_vcf_df,\n",
    "        \"easy_graph\": easy_graph_vcf_df,\n",
    "        \"segdup_graph\": segdup_graph_vcf_df,\n",
    "        \"diff_graph\": diff_graph_vcf_df,\n",
    "    }\n",
    "\n",
    "def get_df_from_vcf(chr_id, not_in_easy_region=False):\n",
    "    rawvcf_path = f\"{raw_vcf_dir}/hprc-v1.1-mc-grch38.raw_{chr_id}.vcf\"\n",
    "    vcfwave_path = f\"{raw_vcf_dir}/hprc-v1.1-mc-grch38.vcfbub.a100k.wave_{chr_id}.vcf\"\n",
    "    graph_vcf_path = f\"{graph_vcf_dir}/graph_{chr_id}{'_no_terminus' if exclude_terminus else ''}.vcf\"\n",
    "\n",
    "    raw_vcf_df = read_vcf_to_dataframe(rawvcf_path)\n",
    "    wave_vcf_df = read_vcf_to_dataframe(vcfwave_path)\n",
    "    graph_vcf_df = read_vcf_to_dataframe(graph_vcf_path)\n",
    "\n",
    "    if not_in_easy_region:\n",
    "        easy_region = get_interval_tree_from_bed(easy_bed, chr_id)\n",
    "\n",
    "    raw_vcf_df = process_raw_vcf(raw_vcf_df)\n",
    "    split_raw_vcf_df = raw_vcf_df\n",
    "\n",
    "    wave_vcf_df = process_raw_vcf(wave_vcf_df)\n",
    "    split_wave_vcf_df = wave_vcf_df\n",
    "\n",
    "    if not_in_easy_region:\n",
    "        split_raw_vcf_df = split_raw_vcf_df[split_raw_vcf_df['POS'].apply(lambda x: len(easy_region[int(x)]) == 0)]\n",
    "        split_wave_vcf_df = split_wave_vcf_df[split_wave_vcf_df['POS'].apply(lambda x: len(easy_region[int(x)]) == 0)]\n",
    "        \n",
    "\n",
    "    graph_vcf_df = process_ourvcf(graph_vcf_df)\n",
    "    simple_graph_vcf_df = graph_vcf_df\n",
    "\n",
    "    if not_in_easy_region:\n",
    "        easy_region = get_interval_tree_from_bed(easy_bed, chr_id)\n",
    "        simple_graph_vcf_df = simple_graph_vcf_df[simple_graph_vcf_df['POS'].apply(lambda x: len(easy_region[int(x)]) == 0)]\n",
    "\n",
    "    return simple_graph_vcf_df, split_raw_vcf_df, split_wave_vcf_df\n",
    "\n",
    "def get_vcf_df_separated_by_regions(chr_id):\n",
    "    # vcfwave_path = f\"{raw_vcf_dir}/hprc-v1.1-mc-grch38.vcfbub.a100k.wave_{chr_id}.vcf\"\n",
    "    graph_vcf_path = f\"{graph_vcf_dir}/graph_{chr_id}{'_no_terminus' if exclude_terminus else ''}.vcf\"\n",
    "\n",
    "    # raw_vcf_df = read_vcf_to_dataframe(vcfwave_path)\n",
    "    graph_vcf_df = read_vcf_to_dataframe(graph_vcf_path)\n",
    "    # graph_vcf_df = process_ourvcf(graph_vcf_df)\n",
    "    simple_graph_vcf_df = graph_vcf_df\n",
    "\n",
    "    region_intervaltree_dict = get_interval_trees_from_bed(TRregion_bed, chr_id)\n",
    "    segdup_region = get_interval_tree_from_bed(segdup_bed, chr_id)\n",
    "\n",
    "    TR_region_dict = {}\n",
    "    for region, tree in region_intervaltree_dict.items():\n",
    "        TR_region_dict[region+\"_segdup\"] = simple_graph_vcf_df[simple_graph_vcf_df['POS'].apply(lambda x: len(tree[int(x)]) > 0 and len(segdup_region[int(x)]) > 0)] \n",
    "        TR_region_dict[region+\"_nonsegdup\"] = simple_graph_vcf_df[simple_graph_vcf_df['POS'].apply(lambda x: len(tree[int(x)]) > 0 and len(segdup_region[int(x)]) == 0)] \n",
    "\n",
    "    TR_region_dict['Unspecified_segdup'] = simple_graph_vcf_df[simple_graph_vcf_df['POS'].apply(lambda x: len([interval for tree in region_intervaltree_dict.values() for interval in tree[int(x)]]) == 0 and len(segdup_region[int(x)]) > 0)]\n",
    "    TR_region_dict['Unspecified_nonsegdup'] = simple_graph_vcf_df[simple_graph_vcf_df['POS'].apply(lambda x: len([interval for tree in region_intervaltree_dict.values() for interval in tree[int(x)]]) == 0 and len(segdup_region[int(x)]) == 0)]\n",
    "    return TR_region_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Region based variant type summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_df_var_summary(region_df):\n",
    "    summary_dict = dict()\n",
    "    for i in range(len(region_df)):\n",
    "        summary_dict[region_df.iloc[i]['Variant_Type']] = summary_dict.get(region_df.iloc[i]['Variant_Type'], 0) + 1\n",
    "    summary_dict['Total'] = len(region_df)\n",
    "    return summary_dict\n",
    "\n",
    "def prepare_dataframe_dict(var_dict):\n",
    "    return pd.DataFrame({\n",
    "        \"Variant Type\": ['SNP', 'MNP', 'Insertion', 'Deletion', 'Replacement', 'Inversion', 'Duplication', 'Total'],\n",
    "        \"Count\": [\n",
    "                  var_dict.get('SNP', 0),\n",
    "                  var_dict.get('MNP', 0),\n",
    "                  var_dict.get('INS', 0),\n",
    "                  var_dict.get('DEL', 0),\n",
    "                  var_dict.get('REP', 0),\n",
    "                  var_dict.get('INV', 0),\n",
    "                  var_dict.get('DUP', 0),\n",
    "                  var_dict.get('Total', 0),\n",
    "        ]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.01s/it]\n"
     ]
    }
   ],
   "source": [
    "easy_list = []\n",
    "segdup_list = []\n",
    "diff_list = []\n",
    "\n",
    "for i in tqdm(chr_list):\n",
    "    var_dicts = get_df_from_vcf_filtered_by_region(f\"chr{i}\", raw_vcf=False)\n",
    "\n",
    "    easy_var_summary = prepare_dataframe_dict(region_df_var_summary(var_dicts['easy_graph']))\n",
    "    segdup_var_summary = prepare_dataframe_dict(region_df_var_summary(var_dicts['segdup_graph']))\n",
    "    diff_var_summary = prepare_dataframe_dict(region_df_var_summary(var_dicts['diff_graph']))\n",
    "\n",
    "    easy_list += easy_var_summary['Count'].to_list()\n",
    "    segdup_list += segdup_var_summary['Count'].to_list()\n",
    "    diff_list += diff_var_summary['Count'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_region_df = pd.DataFrame({\n",
    "    'CHROM': [f'chr{int(i/8) + 1}' for i in range(8*num_chr)],\n",
    "    'Variant_type': ['SNP', 'MNP', 'Insertion', 'Deletion', 'Replacement', 'Inversion', 'Duplication', 'Total']*num_chr,\n",
    "    'Easy_region': easy_list,\n",
    "    'Segdup_region': segdup_list,\n",
    "    'Difficult_region': diff_list,\n",
    "})\n",
    "\n",
    "# var_region_df = pd.DataFrame({\n",
    "#     'CHROM': [f'chr{int(i/8) + 1}' for i in range(8)],\n",
    "#     'Variant_type': ['SNP', 'MNP', 'Insertion', 'Deletion', 'Replacement', 'Inversion', 'Repeat', 'Total'],\n",
    "#     'Easy_region': easy_list,\n",
    "#     'Segdup_region': segdup_list,\n",
    "#     'Difficult_region': diff_list,\n",
    "# })\n",
    "\n",
    "all_df = var_region_df.drop(columns=['CHROM']).groupby('Variant_type', as_index=False).sum()\n",
    "all_df = all_df.set_index('Variant_type').reindex(['SNP', 'MNP', 'Insertion', 'Deletion', 'Replacement', 'Inversion', 'Duplication', 'Total']).reset_index()\n",
    "all_df['CHROM'] = 'all'\n",
    "\n",
    "var_region_concated_df = pd.concat([var_region_df, all_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_region_concated_df.to_csv(f\"{data_vis_dir}/variant_summary_by_region_{chr_set}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Region based SNP comparison between ourvcf and vcfwave/rawvcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:22<00:00, 22.96s/it]\n"
     ]
    }
   ],
   "source": [
    "complex_count_list_raw = []\n",
    "complex_count_list_wave = []\n",
    "nonlinear_count_list = []\n",
    "\n",
    "shared_count_list_raw = []\n",
    "shared_count_list_wave = []\n",
    "\n",
    "our_count_list_raw = []\n",
    "our_count_list_wave = []\n",
    "raw_count_list = []\n",
    "wave_count_list = []\n",
    "\n",
    "for i in tqdm(chr_list):\n",
    "    var_dicts = get_df_from_vcf_filtered_by_region(f\"chr{i}\")\n",
    "    def snp_raw(x):\n",
    "        return len(x['REF']) == 1 and len(x['ALT']) == 1\n",
    "    def snp_graph(x):\n",
    "        return x['Variant_Type'] == 'SNP'\n",
    "    snp_dicts = {k:v[v.apply(snp_raw, axis=1)] if k.split('_')[1] != 'graph' else v[v.apply(snp_graph, axis=1)] for k, v in var_dicts.items()}\n",
    "\n",
    "    easy_linear = snp_dicts['easy_graph'][snp_dicts['easy_graph']['Linear'].apply(lambda x: x == 1)]\n",
    "    easy_nonlinear = snp_dicts['easy_graph'][snp_dicts['easy_graph']['Linear'].apply(lambda x: x == 0)]\n",
    "    easy_simple_raw = pd.merge(snp_dicts['easy_graph'], snp_dicts['easy_raw'], how='inner', on=['#CHROM', 'POS', 'REF', 'ALT'])\n",
    "    easy_simple_wave = pd.merge(snp_dicts['easy_graph'], snp_dicts['easy_wave'], how='inner', on=['#CHROM', 'POS', 'REF', 'ALT'])\n",
    "    \n",
    "    easy_simple_count_raw = len(easy_simple_raw)\n",
    "    easy_complex_count_raw = len(easy_linear) - easy_simple_count_raw\n",
    "    easy_simple_count_wave = len(easy_simple_wave)\n",
    "    easy_complex_count_wave = len(easy_linear) - easy_simple_count_wave\n",
    "    easy_nonlinear_count = len(easy_nonlinear)\n",
    "\n",
    "    easy_shared_count_raw = easy_simple_count_raw\n",
    "    easy_our_count_raw = len(snp_dicts['easy_graph']) - easy_shared_count_raw\n",
    "    easy_raw_count = len(snp_dicts['easy_raw']) - easy_shared_count_raw\n",
    "    easy_shared_count_wave = easy_simple_count_wave\n",
    "    easy_our_count_wave = len(snp_dicts['easy_graph']) - easy_shared_count_wave\n",
    "    easy_wave_count = len(snp_dicts['easy_wave']) - easy_shared_count_wave\n",
    "\n",
    "    segdup_linear = snp_dicts['segdup_graph'][snp_dicts['segdup_graph']['Linear'].apply(lambda x: x == 1)]\n",
    "    segdup_nonlinear = snp_dicts['segdup_graph'][snp_dicts['segdup_graph']['Linear'].apply(lambda x: x == 0)]\n",
    "    segdup_simple_raw = pd.merge(snp_dicts['segdup_graph'], snp_dicts['segdup_raw'], how='inner', on=['#CHROM', 'POS', 'REF', 'ALT'])\n",
    "    segdup_simple_wave = pd.merge(snp_dicts['segdup_graph'], snp_dicts['segdup_wave'], how='inner', on=['#CHROM', 'POS', 'REF', 'ALT'])\n",
    "    \n",
    "    segdup_simple_count_raw = len(segdup_simple_raw)\n",
    "    segdup_complex_count_raw = len(segdup_linear) - segdup_simple_count_raw\n",
    "    segdup_simple_count_wave = len(segdup_simple_wave)\n",
    "    segdup_complex_count_wave = len(segdup_linear) - segdup_simple_count_wave\n",
    "    segdup_nonlinear_count = len(segdup_nonlinear)\n",
    "\n",
    "    segdup_shared_count_raw = segdup_simple_count_raw\n",
    "    segdup_our_count_raw = len(snp_dicts['segdup_graph']) - segdup_shared_count_raw\n",
    "    segdup_raw_count = len(snp_dicts['segdup_raw']) - segdup_shared_count_raw\n",
    "    segdup_shared_count_wave = segdup_simple_count_wave\n",
    "    segdup_our_count_wave = len(snp_dicts['segdup_graph']) - segdup_shared_count_wave\n",
    "    segdup_wave_count = len(snp_dicts['segdup_wave']) - segdup_shared_count_wave\n",
    "\n",
    "    diff_linear = snp_dicts['diff_graph'][snp_dicts['diff_graph']['Linear'].apply(lambda x: x == 1)]\n",
    "    diff_nonlinear = snp_dicts['diff_graph'][snp_dicts['diff_graph']['Linear'].apply(lambda x: x == 0)]\n",
    "    diff_simple_raw = pd.merge(snp_dicts['diff_graph'], snp_dicts['diff_raw'], how='inner', on=['#CHROM', 'POS', 'REF', 'ALT'])\n",
    "    diff_simple_wave = pd.merge(snp_dicts['diff_graph'], snp_dicts['diff_wave'], how='inner', on=['#CHROM', 'POS', 'REF', 'ALT'])\n",
    "    \n",
    "    diff_simple_count_raw = len(diff_simple_raw)\n",
    "    diff_complex_count_raw = len(diff_linear) - diff_simple_count_raw\n",
    "    diff_simple_count_wave = len(diff_simple_wave)\n",
    "    diff_complex_count_wave = len(diff_linear) - diff_simple_count_wave\n",
    "    diff_nonlinear_count = len(diff_nonlinear)\n",
    "\n",
    "    diff_shared_count_raw = diff_simple_count_raw\n",
    "    diff_our_count_raw = len(snp_dicts['diff_graph']) - diff_shared_count_raw\n",
    "    diff_raw_count = len(snp_dicts['diff_raw']) - diff_shared_count_raw\n",
    "    diff_shared_count_wave = diff_simple_count_wave\n",
    "    diff_our_count_wave = len(snp_dicts['diff_graph']) - diff_shared_count_wave\n",
    "    diff_wave_count = len(snp_dicts['diff_wave']) - diff_shared_count_wave\n",
    "\n",
    "\n",
    "    complex_counts_raw = [easy_complex_count_raw, segdup_complex_count_raw, diff_complex_count_raw]\n",
    "    complex_counts_wave = [easy_complex_count_wave, segdup_complex_count_wave, diff_complex_count_wave]\n",
    "    nonlinear_counts = [easy_nonlinear_count, segdup_nonlinear_count, diff_nonlinear_count]\n",
    "\n",
    "    complex_count_list_raw += complex_counts_raw\n",
    "    complex_count_list_wave += complex_counts_wave\n",
    "    nonlinear_count_list += nonlinear_counts\n",
    "\n",
    "    shared_counts_raw = [easy_shared_count_raw, segdup_shared_count_raw, diff_shared_count_raw]\n",
    "    our_counts_raw = [easy_our_count_raw, segdup_our_count_raw, diff_our_count_raw]\n",
    "    raw_counts = [easy_raw_count, segdup_raw_count, diff_raw_count]\n",
    "\n",
    "    shared_counts_wave = [easy_shared_count_wave, segdup_shared_count_wave, diff_shared_count_wave]\n",
    "    our_counts_wave = [easy_our_count_wave, segdup_our_count_wave, diff_our_count_wave]\n",
    "    wave_counts = [easy_wave_count, segdup_wave_count, diff_wave_count]\n",
    "\n",
    "    shared_count_list_raw += shared_counts_raw\n",
    "    our_count_list_raw += our_counts_raw\n",
    "    raw_count_list += raw_counts\n",
    "\n",
    "    shared_count_list_wave += shared_counts_wave\n",
    "    our_count_list_wave += our_counts_wave\n",
    "    wave_count_list += wave_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_df = pd.DataFrame({\n",
    "    'CHROM': [f'chr{chr_list[int(i/3)]}' for i in range(3*num_chr)],\n",
    "    'Region': ['easy', 'segdup', 'difficult']*num_chr,\n",
    "    'Shared_raw': shared_count_list_raw,\n",
    "    'Shared_wave': shared_count_list_wave,\n",
    "    'Ourvcf_only_raw': our_count_list_raw,\n",
    "    'Ourvcf_only_wave': our_count_list_wave,\n",
    "    'Rawvcf_only': raw_count_list,\n",
    "    'Vcfwave_only': wave_count_list,\n",
    "    'Ourvcf_linear_raw': complex_count_list_raw,\n",
    "    'Ourvcf_linear_wave': complex_count_list_wave,\n",
    "    'Ourvcf_offlinear': nonlinear_count_list\n",
    "})\n",
    "\n",
    "all_df = region_df.drop(columns=['CHROM']).groupby('Region', as_index=False).sum()\n",
    "all_df = all_df.set_index('Region').reindex(['easy', 'segdup', 'difficult']).reset_index()\n",
    "all_df['CHROM'] = 'all'\n",
    "\n",
    "region_concated_df = pd.concat([region_df, all_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_concated_df.to_csv(f\"{data_vis_dir}/snp_region_summary_ourvcf_vs_vcfwave_{chr_set}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHROM</th>\n",
       "      <th>Region</th>\n",
       "      <th>Shared_raw</th>\n",
       "      <th>Shared_wave</th>\n",
       "      <th>Ourvcf_only_raw</th>\n",
       "      <th>Ourvcf_only_wave</th>\n",
       "      <th>Rawvcf_only</th>\n",
       "      <th>Vcfwave_only</th>\n",
       "      <th>Ourvcf_linear_raw</th>\n",
       "      <th>Ourvcf_linear_wave</th>\n",
       "      <th>Ourvcf_offlinear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chrY</td>\n",
       "      <td>easy</td>\n",
       "      <td>3196</td>\n",
       "      <td>3204</td>\n",
       "      <td>37</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chrY</td>\n",
       "      <td>segdup</td>\n",
       "      <td>12832</td>\n",
       "      <td>12226</td>\n",
       "      <td>1506</td>\n",
       "      <td>2112</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>258</td>\n",
       "      <td>864</td>\n",
       "      <td>1248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chrY</td>\n",
       "      <td>difficult</td>\n",
       "      <td>11435</td>\n",
       "      <td>8565</td>\n",
       "      <td>13196</td>\n",
       "      <td>16066</td>\n",
       "      <td>0</td>\n",
       "      <td>2819</td>\n",
       "      <td>1544</td>\n",
       "      <td>4414</td>\n",
       "      <td>11652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "      <td>easy</td>\n",
       "      <td>3196</td>\n",
       "      <td>3204</td>\n",
       "      <td>37</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all</td>\n",
       "      <td>segdup</td>\n",
       "      <td>12832</td>\n",
       "      <td>12226</td>\n",
       "      <td>1506</td>\n",
       "      <td>2112</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>258</td>\n",
       "      <td>864</td>\n",
       "      <td>1248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all</td>\n",
       "      <td>difficult</td>\n",
       "      <td>11435</td>\n",
       "      <td>8565</td>\n",
       "      <td>13196</td>\n",
       "      <td>16066</td>\n",
       "      <td>0</td>\n",
       "      <td>2819</td>\n",
       "      <td>1544</td>\n",
       "      <td>4414</td>\n",
       "      <td>11652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CHROM     Region  Shared_raw  Shared_wave  Ourvcf_only_raw  \\\n",
       "0  chrY       easy        3196         3204               37   \n",
       "1  chrY     segdup       12832        12226             1506   \n",
       "2  chrY  difficult       11435         8565            13196   \n",
       "0   all       easy        3196         3204               37   \n",
       "1   all     segdup       12832        12226             1506   \n",
       "2   all  difficult       11435         8565            13196   \n",
       "\n",
       "   Ourvcf_only_wave  Rawvcf_only  Vcfwave_only  Ourvcf_linear_raw  \\\n",
       "0                29            0             6                  9   \n",
       "1              2112            0           122                258   \n",
       "2             16066            0          2819               1544   \n",
       "0                29            0             6                  9   \n",
       "1              2112            0           122                258   \n",
       "2             16066            0          2819               1544   \n",
       "\n",
       "   Ourvcf_linear_wave  Ourvcf_offlinear  \n",
       "0                   1                28  \n",
       "1                 864              1248  \n",
       "2                4414             11652  \n",
       "0                   1                28  \n",
       "1                 864              1248  \n",
       "2                4414             11652  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_concated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variant summary based on allele count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_in_easy_region=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:21<00:00, 21.61s/it]\n"
     ]
    }
   ],
   "source": [
    "simple_count_list_raw = []\n",
    "complex_count_list_raw = []\n",
    "simple_count_list_wave = []\n",
    "complex_count_list_wave = []\n",
    "\n",
    "our_count_list_raw = []\n",
    "our_count_list_wave = []\n",
    "nonlinear_count_list = []\n",
    "\n",
    "def snp_list_by_ac(chr_id):\n",
    "    graph_vcf, raw_vcf, wave_vcf = get_df_from_vcf(chr_id, not_in_easy_region)\n",
    "    \n",
    "    graph_vcf = graph_vcf[graph_vcf['Variant_Type'] == 'SNP']\n",
    "\n",
    "    singleton = graph_vcf[graph_vcf['AC'] == 1]\n",
    "    singleton_linear = singleton[singleton['Linear'] == 1]\n",
    "    singleton_nonlinear = singleton[singleton['Linear'] == 0]\n",
    "\n",
    "    two2four = graph_vcf[(graph_vcf['AC'] >=2) & (graph_vcf['AC'] <=4)]\n",
    "    two2four_linear = two2four[two2four['Linear'] == 1]\n",
    "    two2four_nonlinear = two2four[two2four['Linear'] == 0]\n",
    "\n",
    "    five2eighteen = graph_vcf[(graph_vcf['AC'] >=5) & (graph_vcf['AC'] <=18)]\n",
    "    five2eighteen_linear = five2eighteen[five2eighteen['Linear'] == 1]\n",
    "    five2eighteen_nonlinear =  five2eighteen[five2eighteen['Linear'] == 0]\n",
    "\n",
    "    above19 = graph_vcf[graph_vcf['AC'] >=19]\n",
    "    above19_linear = above19[above19['Linear'] == 1]\n",
    "    above19_nonlinear = above19[above19['Linear'] == 0]\n",
    "\n",
    "    singleton_simple_raw = pd.merge(singleton_linear, raw_vcf, on=['#CHROM', 'POS', 'REF', 'ALT'])\n",
    "    two2four_simple_raw = pd.merge(two2four_linear, raw_vcf, on=['#CHROM', 'POS', 'REF', 'ALT'])\n",
    "    five2eighteen_simple_raw = pd.merge(five2eighteen_linear, raw_vcf, on=['#CHROM', 'POS', 'REF', 'ALT'])\n",
    "    above19_simple_raw = pd.merge(above19_linear, raw_vcf, on=['#CHROM', 'POS', 'REF', 'ALT'])\n",
    "    singleton_simple_wave = pd.merge(singleton_linear, wave_vcf, on=['#CHROM', 'POS', 'REF', 'ALT'])\n",
    "    two2four_simple_wave = pd.merge(two2four_linear, wave_vcf, on=['#CHROM', 'POS', 'REF', 'ALT'])\n",
    "    five2eighteen_simple_wave = pd.merge(five2eighteen_linear, wave_vcf, on=['#CHROM', 'POS', 'REF', 'ALT'])\n",
    "    above19_simple_wave = pd.merge(above19_linear, wave_vcf, on=['#CHROM', 'POS', 'REF', 'ALT'])\n",
    "\n",
    "    singleton_simple_count_raw = len(singleton_simple_raw)\n",
    "    singleton_our_count_raw = len(singleton) - singleton_simple_count_raw\n",
    "    singleton_complex_count_raw = len(singleton_linear) - singleton_simple_count_raw\n",
    "    singleton_simple_count_wave = len(singleton_simple_wave)\n",
    "    singleton_our_count_wave = len(singleton) - singleton_simple_count_wave\n",
    "    singleton_complex_count_wave = len(singleton_linear) - singleton_simple_count_wave\n",
    "    singleton_nonlinear_count = len(singleton_nonlinear)\n",
    "\n",
    "    two2four_simple_count_raw = len(two2four_simple_raw)\n",
    "    two2four_our_count_raw = len(two2four) - two2four_simple_count_raw\n",
    "    two2four_complex_count_raw = len(two2four_linear) - two2four_simple_count_raw\n",
    "    two2four_simple_count_wave = len(two2four_simple_wave)\n",
    "    two2four_our_count_wave = len(two2four) - two2four_simple_count_wave\n",
    "    two2four_complex_count_wave = len(two2four_linear) - two2four_simple_count_wave\n",
    "    two2four_nonlinear_count = len(two2four_nonlinear)\n",
    "\n",
    "    five2eighteen_simple_count_raw = len(five2eighteen_simple_raw)\n",
    "    five2eighteen_our_count_raw = len(five2eighteen) - five2eighteen_simple_count_raw\n",
    "    five2eighteen_complex_count_raw = len(five2eighteen_linear) - five2eighteen_simple_count_raw\n",
    "    five2eighteen_simple_count_wave = len(five2eighteen_simple_wave)\n",
    "    five2eighteen_our_count_wave = len(five2eighteen) - five2eighteen_simple_count_wave\n",
    "    five2eighteen_complex_count_wave = len(five2eighteen_linear) - five2eighteen_simple_count_wave\n",
    "    five2eighteen_nonlinear_count = len(five2eighteen_nonlinear)\n",
    "\n",
    "    above19_simple_count_raw = len(above19_simple_raw)\n",
    "    above19_our_count_raw = len(above19) - above19_simple_count_raw\n",
    "    above19_complex_count_raw = len(above19_linear) - above19_simple_count_raw\n",
    "    above19_simple_count_wave = len(above19_simple_wave)\n",
    "    above19_our_count_wave = len(above19) - above19_simple_count_wave\n",
    "    above19_complex_count_wave = len(above19_linear) - above19_simple_count_wave\n",
    "    above19_nonlinear_count = len(above19_nonlinear)\n",
    "\n",
    "    simple_counts_raw = [singleton_simple_count_raw, two2four_simple_count_raw, \n",
    "                         five2eighteen_simple_count_raw, above19_simple_count_raw]\n",
    "    simple_counts_wave = [singleton_simple_count_wave, two2four_simple_count_wave, \n",
    "                         five2eighteen_simple_count_wave, above19_simple_count_wave]\n",
    "    complex_counts_raw = [singleton_complex_count_raw, two2four_complex_count_raw, \n",
    "                          five2eighteen_complex_count_raw, above19_complex_count_raw]\n",
    "    complex_counts_wave = [singleton_complex_count_wave, two2four_complex_count_wave, \n",
    "                          five2eighteen_complex_count_wave, above19_complex_count_wave]\n",
    "    our_counts_raw = [singleton_our_count_raw, two2four_our_count_raw,\n",
    "                      five2eighteen_our_count_raw, above19_our_count_raw]\n",
    "    our_counts_wave = [singleton_our_count_wave, two2four_our_count_wave,\n",
    "                      five2eighteen_our_count_wave, above19_our_count_wave]\n",
    "    nonlinear_counts = [singleton_nonlinear_count, two2four_nonlinear_count, five2eighteen_nonlinear_count, above19_nonlinear_count]\n",
    "\n",
    "    return simple_counts_raw, simple_counts_wave, complex_counts_raw, complex_counts_wave, our_counts_raw, our_counts_wave, nonlinear_counts\n",
    "\n",
    "for i in tqdm(chr_list):\n",
    "    simple_counts_raw, simple_counts_wave, complex_counts_raw, complex_counts_wave, our_counts_raw, our_counts_wave, nonlinear_counts = snp_list_by_ac(f\"chr{i}\")\n",
    "    simple_count_list_raw += simple_counts_raw\n",
    "    complex_count_list_raw += complex_counts_raw\n",
    "    simple_count_list_wave += simple_counts_wave\n",
    "    complex_count_list_wave += complex_counts_wave\n",
    "    our_count_list_raw += our_counts_raw\n",
    "    our_count_list_wave += our_counts_wave\n",
    "    nonlinear_count_list += nonlinear_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_df = pd.DataFrame({\n",
    "    'CHROM': [f'chr{chr_list[int(i/4)]}' for i in range(4*num_chr)],\n",
    "    'Range': [\"1\", \"2-4\", \"5-18\", \"19+\"]*num_chr,\n",
    "    'Shared_raw': simple_count_list_raw,\n",
    "    'Shared_wave': simple_count_list_wave,\n",
    "    'Ourvcf_raw': our_count_list_raw,\n",
    "    'Ourvcf_wave': our_count_list_wave,\n",
    "    'Ourvcf_linear_raw': complex_count_list_raw,\n",
    "    'Ourvcf_linear_wave': complex_count_list_wave,\n",
    "    'Ourvcf_offlinear': nonlinear_count_list,\n",
    "})\n",
    "\n",
    "all_df = range_df.drop(columns=['CHROM']).groupby('Range', as_index=False).sum()\n",
    "all_df = all_df.set_index('Range').reindex([\"1\", \"2-4\", \"5-18\", \"19+\"]).reset_index()\n",
    "all_df['CHROM'] = 'all'\n",
    "\n",
    "range_concated_df = pd.concat([range_df, all_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_concated_df.to_csv(f\"{data_vis_dir}/snp_ac_range_summary_ourvcf_vs_vcfwave_{chr_set}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHROM</th>\n",
       "      <th>Range</th>\n",
       "      <th>Shared_raw</th>\n",
       "      <th>Shared_wave</th>\n",
       "      <th>Ourvcf_raw</th>\n",
       "      <th>Ourvcf_wave</th>\n",
       "      <th>Ourvcf_linear_raw</th>\n",
       "      <th>Ourvcf_linear_wave</th>\n",
       "      <th>Ourvcf_offlinear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chrY</td>\n",
       "      <td>1</td>\n",
       "      <td>10134</td>\n",
       "      <td>8339</td>\n",
       "      <td>7566</td>\n",
       "      <td>9361</td>\n",
       "      <td>635</td>\n",
       "      <td>2430</td>\n",
       "      <td>6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chrY</td>\n",
       "      <td>2-4</td>\n",
       "      <td>10739</td>\n",
       "      <td>9375</td>\n",
       "      <td>5979</td>\n",
       "      <td>7343</td>\n",
       "      <td>988</td>\n",
       "      <td>2352</td>\n",
       "      <td>4991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chrY</td>\n",
       "      <td>5-18</td>\n",
       "      <td>3394</td>\n",
       "      <td>3077</td>\n",
       "      <td>1157</td>\n",
       "      <td>1474</td>\n",
       "      <td>179</td>\n",
       "      <td>496</td>\n",
       "      <td>978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chrY</td>\n",
       "      <td>19+</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "      <td>1</td>\n",
       "      <td>10134</td>\n",
       "      <td>8339</td>\n",
       "      <td>7566</td>\n",
       "      <td>9361</td>\n",
       "      <td>635</td>\n",
       "      <td>2430</td>\n",
       "      <td>6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all</td>\n",
       "      <td>2-4</td>\n",
       "      <td>10739</td>\n",
       "      <td>9375</td>\n",
       "      <td>5979</td>\n",
       "      <td>7343</td>\n",
       "      <td>988</td>\n",
       "      <td>2352</td>\n",
       "      <td>4991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all</td>\n",
       "      <td>5-18</td>\n",
       "      <td>3394</td>\n",
       "      <td>3077</td>\n",
       "      <td>1157</td>\n",
       "      <td>1474</td>\n",
       "      <td>179</td>\n",
       "      <td>496</td>\n",
       "      <td>978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>all</td>\n",
       "      <td>19+</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CHROM Range  Shared_raw  Shared_wave  Ourvcf_raw  Ourvcf_wave  \\\n",
       "0  chrY     1       10134         8339        7566         9361   \n",
       "1  chrY   2-4       10739         9375        5979         7343   \n",
       "2  chrY  5-18        3394         3077        1157         1474   \n",
       "3  chrY   19+           0            0           0            0   \n",
       "0   all     1       10134         8339        7566         9361   \n",
       "1   all   2-4       10739         9375        5979         7343   \n",
       "2   all  5-18        3394         3077        1157         1474   \n",
       "3   all   19+           0            0           0            0   \n",
       "\n",
       "   Ourvcf_linear_raw  Ourvcf_linear_wave  Ourvcf_offlinear  \n",
       "0                635                2430              6931  \n",
       "1                988                2352              4991  \n",
       "2                179                 496               978  \n",
       "3                  0                   0                 0  \n",
       "0                635                2430              6931  \n",
       "1                988                2352              4991  \n",
       "2                179                 496               978  \n",
       "3                  0                   0                 0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range_concated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotate variants based on the TR region bed file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "def merge_dfs(dfs):\n",
    "    int_cols = dfs[0].select_dtypes(include='int').columns\n",
    "    str_cols = dfs[0].select_dtypes(exclude='int').columns[1:]\n",
    "\n",
    "    summed_ints = reduce(lambda x, y: x + y, [df[int_cols] for df in dfs])\n",
    "    result = pd.concat([dfs[0][str_cols].reset_index(drop=True), summed_ints.reset_index(drop=True)], axis=1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr_list = list(range(1, 23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [2:41:08<00:00, 439.47s/it]  \n"
     ]
    }
   ],
   "source": [
    "TR_regions_dicts = defaultdict(list)\n",
    "\n",
    "for i in tqdm(chr_list):\n",
    "    TR_regions_dict = get_vcf_df_separated_by_regions(f\"chr{i}\")\n",
    "    TR_regions_dict = {region: comprehensive_summary(df, f\"chr{i}\")\n",
    "                       for region, df in TR_regions_dict.items()}\n",
    "    \n",
    "    for region, var_df in TR_regions_dict.items():\n",
    "        TR_regions_dicts[region].append(var_df)\n",
    "\n",
    "TR_regions_dict_all = {region: merge_dfs(var_dfs) for region, var_dfs in TR_regions_dicts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"{var_summary_dir}/TR_regions_var_comprehensive_summary_all_chrs_{chr_set}.xlsx\"\n",
    "\n",
    "sheets = TR_regions_dict_all\n",
    "\n",
    "with pd.ExcelWriter(file_path, engine=\"openpyxl\") as writer:\n",
    "    for sheet, df in sheets.items():\n",
    "        df.to_excel(writer, sheet_name=sheet, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Triallelic/Multiallelic bubble analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generating table for mapping bubble id to within variant edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chr1...\n",
      "Assigning node to bubbles...\n",
      "Conducting bubble summary...\n",
      "Writing bubble summary to CSV...\n",
      "Processing chr2...\n",
      "Assigning node to bubbles...\n",
      "Conducting bubble summary...\n",
      "Writing bubble summary to CSV...\n",
      "Processing chr3...\n",
      "Assigning node to bubbles...\n",
      "Conducting bubble summary...\n",
      "Writing bubble summary to CSV...\n",
      "Processing chr4...\n",
      "Assigning node to bubbles...\n",
      "Conducting bubble summary...\n",
      "Writing bubble summary to CSV...\n",
      "Processing chr5...\n",
      "Assigning node to bubbles...\n",
      "Conducting bubble summary...\n",
      "Writing bubble summary to CSV...\n",
      "Processing chr6...\n",
      "Assigning node to bubbles...\n",
      "Conducting bubble summary...\n",
      "Writing bubble summary to CSV...\n",
      "Processing chr7...\n",
      "Assigning node to bubbles...\n",
      "Conducting bubble summary...\n",
      "Writing bubble summary to CSV...\n",
      "Processing chr8...\n",
      "Assigning node to bubbles...\n",
      "Conducting bubble summary...\n",
      "Writing bubble summary to CSV...\n",
      "Processing chr9...\n",
      "Assigning node to bubbles...\n",
      "Conducting bubble summary...\n",
      "Writing bubble summary to CSV...\n",
      "Processing chr10...\n",
      "Assigning node to bubbles...\n",
      "Conducting bubble summary...\n",
      "Writing bubble summary to CSV...\n",
      "Processing chr11...\n",
      "Assigning node to bubbles...\n",
      "Conducting bubble summary...\n",
      "Writing bubble summary to CSV...\n",
      "Processing chr12...\n",
      "Assigning node to bubbles...\n",
      "Conducting bubble summary...\n",
      "Writing bubble summary to CSV...\n",
      "Processing chr13...\n",
      "Assigning node to bubbles...\n",
      "Conducting bubble summary...\n",
      "Writing bubble summary to CSV...\n",
      "Processing chr14...\n",
      "Assigning node to bubbles...\n",
      "Conducting bubble summary...\n",
      "Writing bubble summary to CSV...\n",
      "Processing chr15...\n",
      "Assigning node to bubbles...\n",
      "Conducting bubble summary...\n",
      "Writing bubble summary to CSV...\n",
      "Processing chr16...\n",
      "Assigning node to bubbles...\n",
      "Conducting bubble summary...\n",
      "Writing bubble summary to CSV...\n",
      "Processing chr17...\n",
      "Assigning node to bubbles...\n",
      "Conducting bubble summary...\n",
      "Writing bubble summary to CSV...\n",
      "Processing chr18...\n",
      "Assigning node to bubbles...\n",
      "Conducting bubble summary...\n",
      "Writing bubble summary to CSV...\n",
      "Processing chr19...\n",
      "Assigning node to bubbles...\n",
      "Conducting bubble summary...\n",
      "Writing bubble summary to CSV...\n",
      "Processing chr20...\n",
      "Assigning node to bubbles...\n",
      "Conducting bubble summary...\n",
      "Writing bubble summary to CSV...\n",
      "Processing chr21...\n",
      "Assigning node to bubbles...\n",
      "Conducting bubble summary...\n",
      "Writing bubble summary to CSV...\n",
      "Processing chr22...\n",
      "Assigning node to bubbles...\n",
      "Conducting bubble summary...\n",
      "Writing bubble summary to CSV...\n"
     ]
    }
   ],
   "source": [
    "for chr in chr_list:\n",
    "    print(f\"Processing chr{chr}...\")\n",
    "    #gfa_path = f\"{gfa_dir}/chr{chr}.gfa\"\n",
    "    snarl_path = f\"{snarl_dir}/chr{chr}.snarls\"\n",
    "    #graph_path = f\"{graph_obj_dir}/chr{chr}.pkl\"\n",
    "    graph_vcf_path = f\"{graph_vcf_dir}/graph_chr{chr}.vcf\"\n",
    "\n",
    "\n",
    "    write_bubble_summary_result(chr_name=f\"chr{chr}\",\n",
    "                                snarl_path=snarl_path,\n",
    "                                vcf_path=graph_vcf_path,\n",
    "                                output_dir=bubble_summary_dir\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Generating table for mapping number of variants to number of alleles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr_list = list(range(1,23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [1:18:32<00:00, 214.22s/it]\n"
     ]
    }
   ],
   "source": [
    "for chr in tqdm(chr_list):\n",
    "    summary_path = f\"{bubble_summary_dir}/bubble_variant_counts_chr{chr}_AT.tsv\"\n",
    "    #graph_path = f\"{graph_obj_dir}/chr{chr}.pkl\"\n",
    "    graph_vcf_path = f\"{graph_vcf_dir}/graph_chr{chr}{'_no_terminus' if exclude_terminus else ''}.vcf\"\n",
    "    vcf_path = f\"{raw_vcf_dir}/hprc-v1.1-mc-grch38.raw_chr{chr}.vcf\"\n",
    "    vcfwave_path = f\"{raw_vcf_dir}/hprc-v1.1-mc-grch38.vcfbub.a100k.wave_chr{chr}.vcf\"\n",
    "\n",
    "    AT_csv = pd.read_csv(summary_path, sep='\\t')\n",
    "    \n",
    "    vcf_dict = {extract_bubble_ids(row['ID'], symbol=True):\n",
    "                    [row['POS'], row['REF'], row['ALT'], \n",
    "                     get_info_dict(row['INFO'])]\n",
    "                for row in read_vcf_line_by_line(graph_vcf_path)}\n",
    "    \n",
    "    AT_bubble_level_dict = {}\n",
    "    AT_allele_count_dict = {}\n",
    "    AT_allele_len_dict = {}\n",
    "    \n",
    "    for row in AT_csv.itertuples():\n",
    "        bubble_id = tuple(sorted(eval(row.Bubble)))\n",
    "        variants = eval(row.Within)\n",
    "        var_len_list = []\n",
    "\n",
    "        AT_bubble_level_dict[bubble_id] = int(row.Level)\n",
    "        AT_allele_count_dict[bubble_id] = len(variants)\n",
    "\n",
    "        for var in variants:\n",
    "            # ref = vcf_dict[var][1] if vcf_dict[var][1] != '.' else vcf_dict[var][3]['NR']\n",
    "            alt = vcf_dict[var][2]\n",
    "            length = len(alt) if alt != '.' else 0\n",
    "            var_len_list.append(length)\n",
    "        AT_allele_len_dict[bubble_id] = ((sum(var_len_list) / len(var_len_list)) if len(var_len_list)!= 0 else 0, \n",
    "                                         max(var_len_list) if len(var_len_list)!= 0 else 0)\n",
    "\n",
    "    vcf_allele_count_dict = {tuple(sorted(extract_bubble_ids(row['ID']))): \n",
    "                             len(row['ALT'].split(',')) \n",
    "                             for row in read_vcf_line_by_line(vcf_path)}\n",
    "    vcf_allele_length_dict = {tuple(sorted(extract_bubble_ids(row['ID']))): \n",
    "                             (sum(map(lambda x: len(x), row['ALT'].split(',')))/len(row['ALT'].split(',')), \n",
    "                              max(map(lambda x: len(x), row['ALT'].split(','))))\n",
    "                             for row in read_vcf_line_by_line(vcf_path)}\n",
    "    \n",
    "\n",
    "    vcfwave_csv = read_vcf_to_dataframe(vcfwave_path)\n",
    "    vcfwave_allele_count_dict = vcfwave_csv['ID'].apply(lambda x: \n",
    "                tuple(sorted(extract_bubble_ids(x.split('_')[0])))).value_counts().to_dict()\n",
    "\n",
    "    bubble_allelic_records = [{'Bubble':tuple(sorted(bubble, key=lambda x: (len(x), x))), \n",
    "                               'Level':AT_bubble_level_dict[bubble], \n",
    "                               'num_variants':AT_allele_count_dict[bubble], \n",
    "                               'avg_allele_length':AT_allele_len_dict[bubble][0],\n",
    "                               'max_allele_length':AT_allele_len_dict[bubble][1],\n",
    "                               'raw_vcf_allele_count': vcf_allele_count_dict.get(bubble, '.'), \n",
    "                               'vcfwave_allele_count':vcfwave_allele_count_dict.get(bubble, '.'),\n",
    "                               'raw_vcf_avg_allele_length': vcf_allele_length_dict.get(bubble, ('.', '.'))[0], \n",
    "                               'raw_vcf_max_allele_length': vcf_allele_length_dict.get(bubble, ('.', '.'))[1], } \n",
    "                               for bubble in AT_allele_count_dict.keys()]\n",
    "    bubble_allelic_df = pd.DataFrame(bubble_allelic_records)\n",
    "    bubble_allelic_df.to_csv(f\"{bubble_summary_dir}/bubble_allele_summary_chr{chr}.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Generating table for extracting triallelic variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chr1...\n",
      "Processing chr2...\n",
      "Processing chr3...\n",
      "Processing chr4...\n",
      "Processing chr5...\n",
      "Processing chr6...\n",
      "Processing chr7...\n",
      "Processing chr8...\n",
      "Processing chr9...\n",
      "Processing chr10...\n",
      "Processing chr11...\n",
      "Processing chr12...\n",
      "Processing chr13...\n",
      "Processing chr14...\n",
      "Processing chr15...\n",
      "Processing chr16...\n",
      "Processing chr17...\n",
      "Processing chr18...\n",
      "Processing chr19...\n",
      "Processing chr20...\n",
      "Processing chr21...\n",
      "Processing chr22...\n"
     ]
    }
   ],
   "source": [
    "triallelic_var_dfs = [] \n",
    "\n",
    "for i in tqdm(chr_list):\n",
    "    graph_vcf_path = f\"{graph_vcf_dir}/graph_chr{i}{'_no_terminus' if exclude_terminus else ''}.vcf\"\n",
    "    graph_vcf_df = read_vcf_to_dataframe(graph_vcf_path)\n",
    "\n",
    "    bubble_summary_path = f\"{bubble_summary_dir}/bubble_variant_counts_chr{i}_AT.tsv\"\n",
    "    bubble_summary_df = pd.read_csv(bubble_summary_path, sep = '\\t')\n",
    "\n",
    "    triallelic_vars = {var for j in range(len(bubble_summary_df)) \n",
    "                       if int(bubble_summary_df.iloc[j]['Level']) == 0 and len(eval(bubble_summary_df.iloc[j]['Within'])) == 2 \n",
    "                       for var in eval(bubble_summary_df.iloc[j]['Within'])}\n",
    "    triallelic_var_df = graph_vcf_df[graph_vcf_df['ID'].apply(lambda x: extract_bubble_ids(x, symbol=True)).isin(triallelic_vars)]\n",
    "\n",
    "    triallelic_var_df.to_csv(f\"{bubble_summary_dir}/triallelic_variants_chr{i}.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'deletion': 356663, 'insertion': 392492, 'snp': 726627, 'mnp': 53508, 'replacement': 17726}\n",
      "1547016\n"
     ]
    }
   ],
   "source": [
    "triallelic_var_summary = {}\n",
    "\n",
    "for i in chr_list:\n",
    "    triallelic_var_path = f\"{bubble_summary_dir}/triallelic_variants_chr{i}.tsv\"\n",
    "    triallelic_var_df = pd.read_csv(triallelic_var_path, sep='\\t')\n",
    "    for j in range(len(triallelic_var_df)):\n",
    "        info_dict = get_info_dict(triallelic_var_df.iloc[j]['INFO'])\n",
    "        VT = info_dict['VT']\n",
    "        triallelic_var_summary[VT] = triallelic_var_summary.get(VT, 0) + 1\n",
    "\n",
    "print(triallelic_var_summary)\n",
    "print(sum(triallelic_var_summary.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.035435075192884594\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "off_linear_count = 0\n",
    "for i in chr_list:\n",
    "    triallelic_path = f\"{bubble_summary_dir}/triallelic_variants_chr{i}.tsv\"\n",
    "    triallelic_df = pd.read_csv(triallelic_path, sep='\\t')\n",
    "\n",
    "    off_linear = (triallelic_df['REF'] == '.').sum()\n",
    "    off_linear_count += off_linear\n",
    "print(off_linear_count / 3472802)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123059"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "off_linear_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Categorize triallelic bubbles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [05:30<00:00, 110.02s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(chr_list):\n",
    "    AT_csv = pd.read_csv(f\"{bubble_summary_dir}/bubble_variant_counts_chr{i}_AT.tsv\", sep='\\t')\n",
    "    gfa_path = f\"{gfa_dir}/chr{i}.gfa\"\n",
    "    pkl_path = f\"{graph_obj_dir}/chr{i}.pkl\"\n",
    "\n",
    "    if os.path.exists(pkl_path):\n",
    "        G = load_graph_from_pkl(pkl_path, compressed=False)\n",
    "    else:\n",
    "        G = PangenomeGraph.from_gfa_line_by_line(gfa_path, compressed=False)\n",
    "        write_dfs_tree_to_gfa(G, f\"{ref_tree_dir}/chr{i}_ref_tree.gfa\")\n",
    "\n",
    "    bubble_candidate = AT_csv.apply(lambda x: len(eval(x['Within'])) == 2 and int(x['Level']) == 0, axis=1)\n",
    "\n",
    "    bubble_ids = AT_csv['Bubble'][bubble_candidate].apply(lambda x: tuple(map(lambda y: y, eval(x))))\n",
    "    bubble_vars = AT_csv['Within'][bubble_candidate].apply(lambda x: list(map(lambda y: y, eval(x))))\n",
    "\n",
    "    bubble_var_dict = dict(zip(bubble_ids, bubble_vars))\n",
    "\n",
    "    failed_bubble = []\n",
    "    error_dict = defaultdict(list)\n",
    "    bubble_type_dict = {}\n",
    "\n",
    "    for bubble, bubble_var in zip(bubble_ids, bubble_vars):\n",
    "        try:\n",
    "            bubble_type_dict[bubble] = G.classify_triallelic_bubble([bubble[0], bubble[1]], bubble_var)\n",
    "        except Exception as e:\n",
    "            failed_bubble.append(bubble)\n",
    "            error_dict['Error'].append(e)\n",
    "            error_dict['Bubble'].append(bubble)\n",
    "    # print(len(bubble_ids), len(failed_bubble))\n",
    "    # print(failed_bubble[:10])\n",
    "    \n",
    "    bubble_triallelic_df = pd.DataFrame({'Bubble': list(bubble_type_dict.keys()), 'Bubble_type': list(bubble_type_dict.values())})\n",
    "    bubble_triallelic_df.to_csv(f\"{bubble_summary_dir}/bubble_triallelic_chr{i}.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Classify superbubbles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr_list = list(range(1,23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [2:31:02<00:00, 411.92s/it]  \n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(chr_list):\n",
    "    AT_path = f\"{bubble_summary_dir}/bubble_variant_counts_chr{i}_AT.tsv\"\n",
    "    vcf_path = f\"{graph_vcf_dir}/graph_chr{i}{'_no_terminus' if exclude_terminus else ''}.vcf\"\n",
    "    # ref_tree_path = f\"{ref_tree_dir}/chr{i}_ref_tree.gfa\"\n",
    "    gfa_path = f\"{gfa_dir}/chr{i}.gfa\"\n",
    "    pkl_path = f\"{graph_obj_dir}/chr{i}.pkl\"\n",
    "\n",
    "    if os.path.exists(pkl_path):\n",
    "        G = load_graph_from_pkl(pkl_path, compressed=False)\n",
    "    else:\n",
    "        G = PangenomeGraph.from_gfa_line_by_line(gfa_path, compressed=False)\n",
    "\n",
    "    AT_df = pd.read_csv(AT_path, sep='\\t')\n",
    "    AT_df = AT_df[AT_df['Level'] == 0]\n",
    "\n",
    "    # vcf_df = read_vcf_to_dataframe(vcf_path)\n",
    "\n",
    "    parent_dict = {v[:-2]: u[:-2] for u, v in G.reference_tree.edges}\n",
    "\n",
    "    vcf_dict = {tuple(sorted(map(lambda x: x[:-2], extract_bubble_ids(row['ID'], symbol=True)))):\n",
    "                    [row['POS'], row['REF'], row['ALT'], \n",
    "                     get_info_dict(row['INFO'])]\n",
    "                for row in read_vcf_line_by_line(vcf_path)}\n",
    "\n",
    "    bubble_pos_list = [] #start node\n",
    "    bubble_list = []\n",
    "    bubble_type_list = []\n",
    "    bubble_max_allele_len_list = []\n",
    "    for row in AT_df.itertuples():\n",
    "        bubble_id = eval(row.Bubble)\n",
    "        variants = eval(row.Within)\n",
    "        variants = {tuple(sorted(map(lambda x: x[:-2], variant))) for variant in variants}\n",
    "        var_len_list = []\n",
    "        pos_list = []\n",
    "\n",
    "        bubble_list.append(bubble_id)\n",
    "\n",
    "        for var in variants:\n",
    "            pos = vcf_dict[var][0]\n",
    "            ref = vcf_dict[var][1] if vcf_dict[var][1] != '.' else vcf_dict[var][3]['NR']\n",
    "            alt = vcf_dict[var][2]\n",
    "            length = (len(ref) if ref != '.' else 0) + (len(alt) if alt != '.' else 0)\n",
    "            var_len_list.append(length)\n",
    "            pos_list.append(pos)\n",
    "        bubble_max_allele_len_list.append(max(var_len_list) if len(var_len_list) != 0 else -1)\n",
    "        bubble_pos_list.append(min(pos_list) if len(pos_list) != 0 else -1)\n",
    "\n",
    "        if parent_dict[bubble_id[0]] == bubble_id[1] or parent_dict[bubble_id[1]] == bubble_id[0]:\n",
    "            bubble_type_list.append('insertion')\n",
    "        elif tuple(sorted(bubble_id)) in variants:\n",
    "            bubble_type_list.append('deletion')\n",
    "        else:\n",
    "            bubble_type_list.append('neither')\n",
    "\n",
    "    bubble_type_df = pd.DataFrame({\"POS\": bubble_pos_list,\n",
    "                                   \"Bubble\": bubble_list,\n",
    "                                   \"Type\": bubble_type_list,\n",
    "                                   \"Max_allele_length\": bubble_max_allele_len_list})\n",
    "    bubble_type_df.to_csv(f\"{bubble_summary_dir}/superbubble_type_chr{i}.tsv\", sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
